import sys
from time import time
import argparse

import file_process
from file_process import file, parse_pdf, file_process_thread
from stem import lemmatizer
import split
import keywords
import shingles
import google
from config import *

if __name__ == "__main__":
    EXTRA_TIME = 0
    START_TIME = time()

    #Command line parsing info
    parser = argparse.ArgumentParser(description = "Searching for plagiarism about a pdf or txt file.")
    parser.add_argument("-p", "--pdf", nargs = 1, help = "Path to pdf file" ''', metavar = "\"*.pdf\""''')
    parser.add_argument("-t", "--txt", nargs = 1, help = "Path to txt file" ''', metavar = "\"*.txt\""''')
    parser.add_argument("-s", "--stopwords", nargs = 1, help = "Path to a file with stopwords")
    parser.add_argument("-l", "--language", nargs = 1, help = "Specifies a language for a text or PDF document", metavar = "ru/en")
    parser.add_argument("-o", "--output", nargs = 1, help = "Path to a file, which will store keywords (if needed)")
    
    if (len(sys.argv) <= 1):
        print(parser.print_help())
        exit(0)
    
    args = parser.parse_args(sys.argv[1:])

    if (args.pdf != None and args.txt != None):
        print("Error in arguments! Only one argument can be specified at the same time:", "pdf filename or txt filename")
        print("Exiting...")
        exit(0)

    if (args.pdf == None) and (args.txt == None):
        print("No file to parse was specified!\n")
        print("Exiting...")
        exit(0)

    input_file = None
    if (args.pdf != None):
        input_file = file(path = args.pdf[0])
    elif (args.txt != None):
        input_file = file(path_txt = args.txt[0])

    stopwords_file = None
    if (args.stopwords != None):
        stopwords_file = file(path_txt = args.stopwords[0])
    else:
        stopwords_file = file()

    output_file = None
    if (args.output != None):
        output_file = file(path_txt = args.output[0])
    else:
        output_file = file()

    lemmatize = None
    if (args.language != None):
        lemmatize = lemmatizer(lang = args.language[0])

    print("input_file = \"{0}\"".format(input_file.path if input_file.path != None else input_file.path_txt))
    print("stopwords_file = \"{0}\"".format(stopwords_file.path_txt if stopwords_file.path_txt != None else ""))
    print("output_file = \"{0}\"".format(output_file.path_txt))
    print("language = {0}".format("\"{0}\"".format(lemmatize.lang) if lemmatize != None else None))
    print()
    
    if (lemmatize != None):
        lemmatize = lemmatize.get()

    input_file.makehash()
    if (input_file.path != None):
        start_time = time()
        retcode = parse_pdf(input_file)
        end_time = time()
        if (retcode == 0):
            print("Took about {0:.3f}s".format(end_time - start_time), end = "\n\n")
        else:
            exit(0)

    """
    Started getting text from input file
    """
    print("Started getting text from \"{0}\"".format(input_file.path_txt))
    start_time = time()
    input_file.text = file_process.get_text(input_file)
    end_time = time()
    print("Getting text from \"{0}\" took {1:.3f}s".format(input_file.path_txt, end_time - start_time), end = "\n\n")

    """
    Started getting keywords from input_file
    """
    stopwords = set()
    if (stopwords_file.path_txt == None):
        pass
    else:
        print("Started parsing TXT with stopwords, wait for a while...")
        start_time = time()
        stopwords = split.get_list(stopwords_file.path_txt, enableComments = True)
        stopwords = set(stopwords)
        end_time = time()
        print("Parsing TXT with stopwords took {0:.3f}s".format(end_time - start_time), end = "\n\n")

    """
    Started getting keyword phrases from input_file
    """
    print("Started getting keyword phrases")
    start_time = time()
    keywords = keywords.getKeyPhrases(input_file.text, stopwords, lemmatizer = lemmatize)
    end_time = time()
    input_file.keywords = keywords
    if len(keywords) == 0:
        print("No keywords were found for an input file \"{0}\"".format(input_file.path_txt))
        exit(0)

    EXTRA_TIME += time()
    if (input("Do you want to print keyword phrases? (\"y\" or \"n\"): ") != "y"):
        pass
    else:
        print("Keyword phrases, generated by RAKE (with a phrase score):", end = "\n\n")
        for phrase in keywords:
            print("\"{0}\" ==> {1:.3f}".format(phrase[0], phrase[1]))
        print()
    EXTRA_TIME -= time()
    print("Getting keyword phrases took {0:.3f}".format(end_time - start_time), "seconds", end = "\n\n")

    if (output_file.path_txt != None):
        out = open(output_file.path_txt, "w")
        for key in keywords:
            out.write("{0}\n".format(key[0]))
        out.close()

    """
    Started getting shingles for input file
    """
    print("Started getting shingles for input file \"{0}\"".format(input_file.path_txt))
    start_time = time()
    input_file.words = file_process.get_words(input_file, stopwords)
    input_file.shingles = shingles.gen_shingles(input_file.words)
    end_time = time()
    if len(input_file.shingles) == 0:
        print("No shingles were built in input file")
        exit(0)
    print("Shingles were built successfully for an input file \"{0}\"".format(input_file.path_txt))
    print("Took about {0:.3f}s".format(end_time - start_time), end = "\n\n")

    """
    Started building query for Google
    """
    query = "("
    for wordphrase in input_file.keywords:
        query += wordphrase[0] + ")+("
    query = query[:-2]
    print("Getting links from Google search")
    start_time = time()
    links = google.search(query, tld = TLD, lang = LANG, pause = QUERY_PAUSE, filetype = FILETYPE)
    end_time = time()
    print("Finding links took {0:.3f}s".format(end_time - start_time), end = "\n\n")

    """
    May lose some links, because Google returns links to sites, which contains urls for the needed PDFs
    You should process them also, to get the needed links
    At this point, I skip them
    """

    number = 1
    for link in links:
        print("{0}) {1}".format(number, link))
        number += 1

    checked_links = []
    number = 0
    for link in links:
        if (link.endswith(".pdf") == True or link.endswith("=pdf") == True):
            number += 1
            checked_links.append(link)
            print("{0}) \"{1}\"".format(number, link))

    links = checked_links
    if len(links) == 0:
        print("No results were found for your request (with a \".pdf\" at the end)")
        print("Exiting...")
        exit(0)

    print("Started processing files", end = "\n\n")
    threads = []
    threadID = 0
    for url in links:
        threadID += 1
        thread = file_process_thread(threadID, url, stopwords, input_file)
        thread.start()
        threads.append(thread)

    files = []
    for t in threads:
        return_file = t.join
        return_file = return_file()
        if (return_file != None):
            files.append(return_file)

    num = 1
    for f in files:
        print("{0}) {1}".format(f.path_txt))
        print(f.similarity, end = "\n\n")
        num += 1

    END_TIME = time()
    print("Program ended successfully!")
    print("Execution time {0:.3f}s".format(END_TIME - START_TIME - abs(EXTRA_TIME)))
